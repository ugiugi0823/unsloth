#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ PEFT (LoRA) ê¸°ë°˜ QA ëª¨ë¸ ê³ íš¨ìœ¨ íŒŒì¸íŠœë‹ - SFTTrainer + DeepSpeed ZeRO

ã€ì£¼ìš” ê°œì„ ì‚¬í•­ã€‘
- SFTTrainer ì‚¬ìš©ìœ¼ë¡œ ëŒ€í™”í˜• ë°ì´í„° ì²˜ë¦¬ ìµœì í™”
- Gemma3 ê³µì‹ Chat Template ì ìš©
- ğŸ†• ê·œì¹™ ê¸°ë°˜ ìˆ˜ë™ ë ˆì´ë¸”ë§ (responseë§Œ í•™ìŠµ)
- ğŸš€ DeepSpeed ZeRO-2/3 ì§€ì›ìœ¼ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ
- ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ì½”ë“œ ê°€ë…ì„± ë° ì¬ì‚¬ìš©ì„± í–¥ìƒ (ëª¨ë¸ ë¡œë“œ ë¡œì§ ë¶„ë¦¬)
- wandb ë¡œê¹… ì§€ì›

ã€ë ˆì´ë¸”ë§ ë°©ì‹ã€‘
- prompt ë¶€ë¶„: labels = -100 (í•™ìŠµ ì•ˆ í•¨)
- response ë¶€ë¶„: labels = token_ids (í•™ìŠµí•¨)

ã€ë°ì´í„° í˜•ì‹ã€‘
- JSONL í¬ë§·: {"qas": [{"question": "...", "answer": "...", "question_type": [...], "difficulty": "..."}]}

ã€ì‚¬ìš© ì˜ˆì‹œã€‘
# ë‹¨ì¼ GPU (4bit ì–‘ìí™”)
python train_deepspeed.py --model /path/to/model --epochs 3 --batch_size 16

# DeepSpeed ë©€í‹° GPU (BF16, ZeRO-2)
deepspeed --num_gpus=4 train_deepspeed.py \\
    --model /path/to/model --deepspeed ds_config_zero2.json \\
    --epochs 3 --batch_size 8

# DeepSpeed ë©€í‹° GPU (BF16, ZeRO-3)
deepspeed --num_gpus=4 train_deepspeed.py \\
    --model /path/to/model --deepspeed ds_config_zero3.json \\
    --epochs 3 --batch_size 8

âš ï¸ ì£¼ì˜: DeepSpeed ZeRO-3 ì‚¬ìš© ì‹œ 4bit ì–‘ìí™” ë° device_mapì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤ (ì¶©ëŒ ë°©ì§€)
"""

import os
import json
import torch
import argparse
import random
import re
import numpy as np
import warnings
from datetime import datetime
from typing import Dict, List, Any, Optional

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore')

from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    TrainerCallback,
    TrainerState,
    TrainerControl
)
from trl import SFTTrainer
from peft import (
    LoraConfig,
    TaskType,
    PeftModel
)

# DeepSpeed ZeRO support í™•ì¸
try:
    from deepspeed import zero
    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False

# wandb support í™•ì¸
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False



# ============================================================================
# DeepSpeed ZeRO Utility Functions
# ============================================================================

def maybe_zero_3(param: torch.Tensor) -> torch.Tensor:
    """DeepSpeed ZeRO-3ì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"""
    if DEEPSPEED_AVAILABLE and hasattr(param, "ds_id"):
        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE
        with zero.GatheredParameters([param]):
            param = param.data.detach().cpu().clone()
    else:
        param = param.detach().cpu().clone()
    return param


def get_peft_state_maybe_zero_3(named_params: List[tuple], bias: str) -> Dict[str, torch.Tensor]:
    """PEFT ëª¨ë¸ ìƒíƒœë¥¼ DeepSpeed ZeRO-3ì™€ í˜¸í™˜ë˜ê²Œ ì €ì¥"""
    # ì›ë³¸ ë¡œì§ ìœ ì§€ (DeepSpeed ìœ í‹¸ë¦¬í‹°)
    if bias == "none":
        to_return = {k: t for k, t in named_params if "lora_" in k}
    elif bias == "all":
        to_return = {k: t for k, t in named_params if "lora_" in k or "bias" in k}
    elif bias == "lora_only":
        to_return = {}
        maybe_lora_bias = {}
        lora_bias_names = set()
        for k, t in named_params:
            if "lora_" in k:
                to_return[k] = t
                bias_name = k.split("lora_")[0] + "bias"
                lora_bias_names.add(bias_name)
            elif "bias" in k:
                maybe_lora_bias[k] = t
        for k, t in maybe_lora_bias.items():
            if k in lora_bias_names:
                to_return[k] = t
    else:
        raise NotImplementedError(f"bias={bias} is not implemented")
    
    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}
    return to_return

# ============================================================================
# Data Collator and Metrics
# ============================================================================

class DataCollatorForCompletionOnly:
    """ë‹µë³€ ë¶€ë¶„ë§Œ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” Custom Data Collator"""
    
    RESPONSE_TEMPLATE = "<start_of_turn>model\n"
    
    def __init__(self, tokenizer: AutoTokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # response_templateì„ í† í°í™”í•˜ì—¬ ID ì‹œí€€ìŠ¤ ì–»ê¸°
        self.response_token_ids = self.tokenizer.encode(
            self.RESPONSE_TEMPLATE, 
            add_special_tokens=False
        )
        print(f"ğŸ“Œ Response Template: '{self.RESPONSE_TEMPLATE}'")
        print(f"ğŸ“Œ Response Token IDs: {self.response_token_ids}")
        print(f"ğŸ“Œ Max Length: {self.max_length}")
        
    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ ë°ì´í„°ë¥¼ collateí•˜ê³  labels ë§ˆìŠ¤í‚¹"""
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ (SFTTrainerì˜ formatting_funcì´ 'text' í‚¤ì— ê²°ê³¼ë¥¼ ì €ì¥)
        texts = [example['text'] for example in examples if 'text' in example]
        
        # í† í°í™” (Gemma3 ìš”êµ¬ì‚¬í•­: token_type_ids ì¶”ê°€)
        batch = self.tokenizer(
            texts,
            truncation=True,
            max_length=self.max_length,
            padding=True,
            return_tensors="pt"
        )
        
        # token_type_idsê°€ ì—†ìœ¼ë©´ ìƒì„± (Gemma3 í•„ìˆ˜)
        if "token_type_ids" not in batch:
            batch["token_type_ids"] = torch.zeros_like(batch["input_ids"])
        
        # Labels ìƒì„± (input_ids ë³µì‚¬)
        labels = batch["input_ids"].clone()
        
        response_len = len(self.response_token_ids)
        
        # ë””ë²„ê¹…ìš© ì¹´ìš´í„°
        found_count = 0
        not_found_count = 0
        
        # ê° ìƒ˜í”Œì— ëŒ€í•´ response ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° ë° ë§ˆìŠ¤í‚¹
        for idx in range(len(labels)):
            input_ids = batch["input_ids"][idx].tolist()
            
            # response_template í† í° ì‹œí€€ìŠ¤ë¥¼ ì°¾ê¸°
            try:
                # í…œí”Œë¦¿ì˜ ì‹œì‘ ì¸ë±ìŠ¤ë¥¼ ì°¾ê³ , í…œí”Œë¦¿ ê¸¸ì´ë§Œí¼ ë”í•´ ì‘ë‹µ ì‹œì‘ì  í™•ë³´
                # Python list index()ëŠ” O(N)ì´ì§€ë§Œ, ì‘ë‹µ ì‹œì‘ì ì€ ë³´í†µ ì‹œí€€ìŠ¤ ì•ìª½ì— ìœ„ì¹˜í•˜ì—¬ ë¹ ë¦„
                template_start_idx = -1
                for i in range(len(input_ids) - response_len + 1):
                    if input_ids[i:i + response_len] == self.response_token_ids:
                        template_start_idx = i
                        break
                
                if template_start_idx != -1:
                    # response_template ì´í›„ë¶€í„° í•™ìŠµ (í…œí”Œë¦¿ ìì²´ëŠ” ì œì™¸)
                    response_start_idx = template_start_idx + response_len
                    labels[idx, :response_start_idx] = -100
                    found_count += 1
                else:
                    # templateì„ ëª» ì°¾ì€ ê²½ìš° ì „ì²´ ë§ˆìŠ¤í‚¹ (í•™ìŠµ ì•ˆ í•¨)
                    labels[idx, :] = -100
                    not_found_count += 1
                    # ì²˜ìŒ 2ê°œë§Œ ë””ë²„ê¹… ì¶œë ¥
                    if not_found_count <= 2:
                        decoded_full = self.tokenizer.decode(input_ids)  # ì „ì²´ í…ìŠ¤íŠ¸
                        print(f"âš ï¸  Response Template ëª» ì°¾ìŒ (ìƒ˜í”Œ {idx})")
                        print(f"   ===== ì „ì²´ í…ìŠ¤íŠ¸ =====")
                        print(decoded_full)
                        print(f"   ===== ë =====")
                        print(f"   ì°¾ëŠ” í…œí”Œë¦¿: {self.RESPONSE_TEMPLATE}")
                        print(f"   í…œí”Œë¦¿ í† í° IDs: {self.response_token_ids}")
                        print(f"   í…ìŠ¤íŠ¸ì— í…œí”Œë¦¿ í¬í•¨ ì—¬ë¶€: {self.RESPONSE_TEMPLATE in decoded_full}")
            
            except Exception as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ ë§ˆìŠ¤í‚¹ (ì•ˆì „ ì¥ì¹˜)
                labels[idx, :] = -100
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ (ìƒ˜í”Œ {idx}): {e}")
            
            # padding í† í°ë„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
            labels[idx][labels[idx] == self.tokenizer.pad_token_id] = -100
        
        # ë°°ì¹˜ í†µê³„ ì¶œë ¥ (ê°€ë”ì”©ë§Œ)
        if not_found_count > 0:
            print(f"ğŸ“Š ë°°ì¹˜ í†µê³„: í…œí”Œë¦¿ ì°¾ìŒ={found_count}, ëª» ì°¾ìŒ={not_found_count}")
        
        batch["labels"] = labels
        
        return batch


# ============================================================================
# Trainer Components
# ============================================================================

class PerplexityLoggingCallback(TrainerCallback):
    """Train/Eval lossë¥¼ perplexityë¡œ ë³€í™˜í•˜ì—¬ ë¡œê¹…í•˜ëŠ” Callback"""
    
    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs: Dict[str, float] = None, **kwargs):
        """ë¡œê¹… ì‹œ perplexity ì¶”ê°€"""
        if logs is None:
            return
        
        # train_lossê°€ ìˆìœ¼ë©´ train_perplexity ê³„ì‚°
        if 'loss' in logs:
            import math
            logs['train_perplexity'] = math.exp(logs['loss'])
        
        # eval_lossê°€ ìˆìœ¼ë©´ eval_perplexity ê³„ì‚°
        if 'eval_loss' in logs:
            import math
            logs['eval_perplexity'] = math.exp(logs['eval_loss'])


class QAPEFTTrainer:
    """PEFTë¥¼ ì‚¬ìš©í•œ QA íŒŒì¸íŠœë‹ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.tokenizer: Optional[AutoTokenizer] = None
        self.model: Optional[Any] = None
        self.dataset: Optional[Dataset] = None
        self.train_dataset: Optional[Dataset] = None
        self.eval_dataset: Optional[Dataset] = None
        self.peft_config: Optional[LoraConfig] = None
        
    def setup_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì • (LoRA Config í¬í•¨)"""
        print(f"ğŸ¤– ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì • ì¤‘: {self.config['model_name']}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['model_name'],
            trust_remote_code=True,
            padding_side="right",
            use_fast=False
        )
        
        # Gemma3 pad_token ì„¤ì • (EOS í† í° ì‚¬ìš© - ZeRO-3 í˜¸í™˜)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        print(f"ğŸ“Œ PAD token ID: {self.tokenizer.pad_token_id}, EOS token ID: {self.tokenizer.eos_token_id}")
        
        # LoRA ì„¤ì •
        self.peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.config['lora_r'],
            lora_alpha=self.config['lora_alpha'],
            lora_dropout=self.config['lora_dropout'],
            target_modules=self.config['target_modules']
        )
        
        print("âœ… í† í¬ë‚˜ì´ì € ë° PEFT ì„¤ì • ì™„ë£Œ")

    @staticmethod
    def _is_valid_example(item: Dict[str, Any]) -> bool:
        """í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸: input.question, output.answer"""
        try:
            if not isinstance(item, dict):
                return False
            inp = item.get('input')
            out = item.get('output')
            if not isinstance(inp, dict) or not isinstance(out, dict):
                return False
            question = inp.get('question')
            answer = out.get('answer')
            if not isinstance(question, str) or not question.strip():
                return False
            if not isinstance(answer, str) or not answer.strip():
                return False
            return True
        except Exception:
            return False

    @staticmethod
    def _normalize_example(item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ì™¸ë¶€ ìŠ¤í‚¤ë§ˆë¥¼ ë‚´ë¶€ í‘œì¤€(input/output)ìœ¼ë¡œ ë³€í™˜
        
        ì§€ì› í˜•ì‹:
        1. {"qas": [{"question": "...", "answer": "...", ...}]}  (í˜„ì¬ ARMS QA ë°ì´í„°)
        2. {"question": "...", "answer": "..."}  (ë‹¨ìˆœ QA í˜•ì‹)
        3. {"input": {"question": "..."}, "output": {"answer": "..."}}  (ë‚´ë¶€ í‘œì¤€)
        """
        if QAPEFTTrainer._is_valid_example(item):
            return item
        
        # ì¼€ì´ìŠ¤1: {"qas": [{"question": "...", "answer": "...", ...}]} (ARMS QA í˜•ì‹)
        if isinstance(item, dict) and 'qas' in item:
            qas_list = item.get('qas', [])
            if not isinstance(qas_list, list) or len(qas_list) == 0:
                return None
            
            # ì²« ë²ˆì§¸ QA ìŒ ì‚¬ìš©
            qa = qas_list[0]
            if not isinstance(qa, dict):
                return None
            
            question = qa.get('question', '')
            answer = qa.get('answer', '')
            
            if not question or not answer:
                return None
            
            return {
                'input': {
                    'question': question
                },
                'output': {
                    'answer': answer
                },
                'metadata': {
                    'question_type': qa.get('question_type', []),
                    'difficulty': qa.get('difficulty', '')
                }
            }
        
        # ì¼€ì´ìŠ¤2: {"question": "...", "answer": "..."} (ë‹¨ìˆœ QA í˜•ì‹)
        if isinstance(item, dict) and 'question' in item and 'answer' in item:
            question = item.get('question', '')
            answer = item.get('answer', '')
            
            if not question or not answer:
                return None
            
            return {
                'input': {
                    'question': question
                },
                'output': {
                    'answer': answer
                },
                'metadata': {
                    'question_type': item.get('question_type', []),
                    'difficulty': item.get('difficulty', '')
                }
            }
        
        return None

    @staticmethod
    def _prepare_qa_data(item: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """QA ë°ì´í„°ì—ì„œ í”„ë¡¬í”„íŠ¸ì— í•„ìš”í•œ ìš”ì†Œë¥¼ ì¤€ë¹„"""
        question = item['input']['question']
        answer = item['output']['answer']
        
        # Gemma3 Chat Template User Content
        user_content = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸:
{question}

---

ê·œì¹™:
1. ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ì—¬ ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ê¸°ìˆ ì ì¸ ìš©ì–´ê°€ ìˆë‹¤ë©´ ì •í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
3. ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."""
        
        assistant_content = answer

        return {
            "user_content": user_content,
            "assistant_content": assistant_content,
            "reference_answer": answer  # ì›ë³¸ ë‹µë³€ (ë©”íŠ¸ë¦­ ê³„ì‚°ìš©)
        }

    def formatting_func(self, example: Dict[str, Any]) -> str:
        """SFTTrainerìš© í¬ë§·íŒ… í•¨ìˆ˜ - ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜ (ë™ì  truncation ì ìš©)"""
        try:
            formatted_data = self._prepare_qa_data(example, self.config)
            
            # Gemma3 í¬ë§·ìœ¼ë¡œ ì¡°í•©
            # promptì— model turn ì‹œì‘ê¹Œì§€ í¬í•¨í•˜ì—¬, assistant_contentë§Œ í•™ìŠµë˜ë„ë¡ í•¨
            user_content = formatted_data['user_content']
            assistant_content = formatted_data['assistant_content']
            
            # ê³ ì • í…œí”Œë¦¿ (ìë¥´ë©´ ì•ˆ ë¨)
            prefix = "<bos><start_of_turn>user\n"
            middle = "<end_of_turn>\n<start_of_turn>model\n"
            suffix = "<end_of_turn>\n"
            
            response = assistant_content + suffix
            
            max_length = self.config.get('max_length', 512)
            
            # ê° ë¶€ë¶„ì˜ í† í° ê¸¸ì´ ê³„ì‚°
            prefix_tokens = self.tokenizer.encode(prefix, add_special_tokens=False)
            middle_tokens = self.tokenizer.encode(middle, add_special_tokens=False)
            response_tokens = self.tokenizer.encode(response, add_special_tokens=False)
            user_content_tokens = self.tokenizer.encode(user_content, add_special_tokens=False)
            
            # ê³ ì • í† í° ê¸¸ì´
            fixed_length = len(prefix_tokens) + len(middle_tokens) + len(response_tokens)
            available_for_user = max_length - fixed_length
            
            # user_contentê°€ ë„ˆë¬´ ê¸¸ë©´ truncation
            if len(user_content_tokens) > available_for_user:
                if available_for_user > 50:  # ìµœì†Œ ê¸¸ì´ í™•ë³´
                    # ì•ë¶€ë¶„ë§Œ ìœ ì§€ (ë’¤ì—ì„œ ìë¦„)
                    truncated_tokens = user_content_tokens[:available_for_user]
                    user_content = self.tokenizer.decode(truncated_tokens, skip_special_tokens=False)
                    
                    # ë””ë²„ê¹… ì¶œë ¥ (ì²˜ìŒ ëª‡ ê°œë§Œ)
                    if not hasattr(self, '_truncation_count'):
                        self._truncation_count = 0
                    if self._truncation_count < 3:
                        print(f"âœ‚ï¸  User content truncated: {len(user_content_tokens)} â†’ {available_for_user} tokens")
                        self._truncation_count += 1
                else:
                    # available ê³µê°„ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ê²½ê³ 
                    if not hasattr(self, '_warning_count'):
                        self._warning_count = 0
                    if self._warning_count < 3:
                        print(f"âš ï¸  Responseê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. max_length ì¦ê°€ í•„ìš”: response={len(response_tokens)}, max={max_length}")
                        self._warning_count += 1
            
            # ìµœì¢… í…ìŠ¤íŠ¸ ì¡°í•©
            full_text = prefix + user_content + middle + response
            
            return full_text
            
        except KeyError as e:
            raise ValueError(f"í•„ìˆ˜ í‚¤ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
    
    def load_and_prepare_dataset(self):
        """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ (ìŠ¤í‚¤ë§ˆ ì •ê·œí™” í¬í•¨)"""
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {self.config['data_path']}")
        
        data = []
        # JSONL ë˜ëŠ” JSON íŒŒì¼ ë¡œë“œ
        if self.config['data_path'].endswith('.jsonl'):
            with open(self.config['data_path'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        data.append(json.loads(line))
        else:
            with open(self.config['data_path'], 'r', encoding='utf-8') as f:
                data = json.load(f)
        
        # ë°ì´í„° ìƒ˜í”Œë§ (ì˜µì…˜)
        if self.config.get('max_samples') and len(data) > self.config['max_samples']:
            data = data[:self.config['max_samples']]
            print(f"ğŸ“‹ ë°ì´í„° ìƒ˜í”Œë§: {len(data)}ê°œ ì‚¬ìš©")
        
        # ìŠ¤í‚¤ë§ˆ ì •ê·œí™” ë° í•„í„°ë§ (í•™ìŠµ ë°ì´í„°)
        normalized_data = []
        converted_cnt = 0
        for ex in data:
            norm = self._normalize_example(ex)
            if norm is None:
                continue
            if norm is not ex:
                converted_cnt += 1
            normalized_data.append(norm)
        dropped = len(data) - len(normalized_data)
        if converted_cnt > 0:
            print(f"ğŸ”§ í•™ìŠµ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìë™ ë³€í™˜: {converted_cnt}ê°œ")
        if dropped > 0:
            print(f"âš ï¸ ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ë¡œ ì œì™¸ëœ í•™ìŠµ ìƒ˜í”Œ: {dropped}ê°œ")
        data = normalized_data
        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(data)}ê°œ")
        
        self.dataset = Dataset.from_list(data)
        
        # ë³„ë„ í‰ê°€ ë°ì´í„° ê²½ë¡œê°€ ì œê³µë˜ë©´ ê·¸ íŒŒì¼ì„ í‰ê°€ ë°ì´í„°ë¡œ ì‚¬ìš© (ì •ê·œí™” í¬í•¨)
        eval_data_path = self.config.get('eval_data_path')
        if eval_data_path and os.path.exists(eval_data_path):
            print(f"ğŸ“Š í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {eval_data_path}")
            eval_data = []
            if eval_data_path.endswith('.jsonl'):
                with open(eval_data_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            eval_data.append(json.loads(line))
            else:
                with open(eval_data_path, 'r', encoding='utf-8') as f:
                    eval_data = json.load(f)

            normalized_eval = []
            converted_eval_cnt = 0
            for ex in eval_data:
                norm = self._normalize_example(ex)
                if norm is None:
                    continue
                if norm is not ex:
                    converted_eval_cnt += 1
                normalized_eval.append(norm)
            dropped_eval = len(eval_data) - len(normalized_eval)
            if converted_eval_cnt > 0:
                print(f"ğŸ”§ í‰ê°€ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìë™ ë³€í™˜: {converted_eval_cnt}ê°œ")
            if dropped_eval > 0:
                print(f"âš ï¸ ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ë¡œ ì œì™¸ëœ í‰ê°€ ìƒ˜í”Œ: {dropped_eval}ê°œ")

            self.train_dataset = self.dataset
            self.eval_dataset = Dataset.from_list(normalized_eval)
            print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(self.train_dataset)}ê°œ, í‰ê°€ ë°ì´í„°: {len(self.eval_dataset)}ê°œ (ì™¸ë¶€ íŒŒì¼)")
        else:
            # í•™ìŠµ/ê²€ì¦ ë¶„í• 
            if self.config['validation_split'] > 0:
                split_dataset = self.dataset.train_test_split(
                    test_size=self.config['validation_split'],
                    seed=42
                )
                self.train_dataset = split_dataset['train']
                self.eval_dataset = split_dataset['test']
                print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(self.train_dataset)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(self.eval_dataset)}ê°œ")
            else:
                self.train_dataset = self.dataset
                self.eval_dataset = None
                print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(self.train_dataset)}ê°œ (ê²€ì¦ ì—†ìŒ)")

    def setup_training_arguments(self, timestamp: str) -> TrainingArguments:
        """í•™ìŠµ ì¸ì ì„¤ì •"""
        output_dir = os.path.join(
            self.config['output_dir'],
            f"qa_peft_{timestamp}"
        )
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=self.config['num_epochs'],
            per_device_train_batch_size=self.config['batch_size'],
            per_device_eval_batch_size=self.config['batch_size'],
            gradient_accumulation_steps=self.config['gradient_accumulation_steps'],
            learning_rate=self.config['learning_rate'],
            warmup_steps=self.config.get('warmup_steps', 0),
            warmup_ratio=self.config.get('warmup_ratio', None),
            logging_steps=self.config['logging_steps'],
            save_steps=self.config['save_steps'],
            eval_steps=self.config['eval_steps'] if self.eval_dataset else None,
            eval_strategy="steps" if self.eval_dataset else "no",
            save_strategy="steps",
            load_best_model_at_end=False,  # DeepSpeed ZeRO-3 í˜¸í™˜ì„±: checkpoint ë¡œë“œ ì‹¤íŒ¨ ë°©ì§€
            metric_for_best_model="eval_loss" if self.eval_dataset else None,
            greater_is_better=False,
            save_only_model=True,  # LoRA adapter ì €ì¥ì„ ìœ„í•´ Trueë¡œ ì„¤ì •
            fp16=False,
            bf16=True,  # BF16 ì‚¬ìš© (Zero-3 ìµœì í™”)
            optim="adamw_torch",
            lr_scheduler_type=self.config.get('lr_scheduler_type', 'cosine'),
            eval_accumulation_steps=4,
            weight_decay=0.01,
            max_grad_norm=1.0,
            save_total_limit=3,
            dataloader_drop_last=True,
            dataloader_num_workers=0,
            group_by_length=True,
            remove_unused_columns=False,
            push_to_hub=False,
            # hub_token=None,  # TRL 0.25.0.dev0 í˜¸í™˜ì„± (push_to_hub_token â†’ hub_token)
            gradient_checkpointing=True,
            deepspeed=self.config.get('deepspeed'),
            local_rank=self.config.get('local_rank', -1),
            report_to="wandb" if (self.config['use_wandb'] and WANDB_AVAILABLE) else None,
            run_name=self.config.get('wandb_run_name', f"rag_peft_{timestamp}") if (self.config['use_wandb'] and WANDB_AVAILABLE) else None
        )
        
        return training_args

    def _load_model(self) -> Any:
        """ëª¨ë¸ ë¡œë“œ ë° DeepSpeed ì„¤ì • ì²˜ë¦¬ (deepspeed_ref.py ë°©ì‹ ì ìš©)"""
        print("ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # ğŸ’¡ í† í¬ë‚˜ì´ì € ì„¤ì •ì´ ì´ë¯¸ ì™„ë£Œë˜ì–´ì•¼ í•¨ (pad_token_id í™•ë³´)
        if self.tokenizer is None:
             raise ValueError("í† í¬ë‚˜ì´ì €ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. setup_model_and_tokenizerë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        # DeepSpeed configì— ë”°ë¼ precision ê²°ì •
        torch_dtype = torch.bfloat16  # BF16 ì‚¬ìš© (Zero-3 ìµœì í™”)
        print("  - BF16 precision (ì–‘ìí™” ì—†ìŒ)")
        
        # ğŸ”§ ZeRO-3 í˜¸í™˜ ëª¨ë¸ ë¡œë“œ (Gemma3ì—ì„œ ê²€ì¦ëœ ë°©ë²•)
        print("  - ëª¨ë¸ ë¡œë“œ ì¤‘ (ZeRO-3 í˜¸í™˜)...")
        
        # ZeRO-3ì—ì„œëŠ” low_cpu_mem_usageì™€ device_mapì„ ë¹„í™œì„±í™” (GPT ê²€ì¦ ë°©ì‹)
        model = AutoModelForCausalLM.from_pretrained(
            self.config['model_name'],
            attn_implementation="eager",
            torch_dtype=torch_dtype,
            trust_remote_code=True,
            low_cpu_mem_usage=False,  # â† ZeRO-3 í•„ìˆ˜! Accelerate ê°„ì„­ ë°©ì§€
            device_map=None,          # â† ZeRO-3 í•„ìˆ˜! DeepSpeedê°€ ì¥ì¹˜ ê´€ë¦¬
        )
        print("  - ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
        # use_cacheë¥¼ configì—ì„œ ì„¤ì •
        model.config.use_cache = False
        print("  - use_cache=False ì„¤ì • ì™„ë£Œ")
        
        # ğŸ”§ LoRA ì ìš© (get_peft_model ì‚¬ìš©)
        print("  - LoRA ì ìš© ì¤‘...")
        from peft import get_peft_model
        model = get_peft_model(model, self.peft_config)
        print("  - LoRA ì ìš© ì™„ë£Œ!")
        
        # ğŸ”§ gradient_checkpointing í™œì„±í™” ì‹œ enable_input_require_grads() í˜¸ì¶œ
        if self.config.get('gradient_checkpointing', True):
            model.enable_input_require_grads()
            print("  - gradient_checkpointing í™œì„±í™”")
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return model
        
    def train(self) -> Dict[str, Any]:
        """ëª¨ë¸ í•™ìŠµ (SFTTrainer ì‚¬ìš©)"""
        print("ğŸš€ í•™ìŠµ ì‹œì‘ (SFTTrainer)")
        
        # í†µì¼ëœ timestamp ìƒì„± (wandb, output_dirì—ì„œ ëª¨ë‘ ì‚¬ìš©)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # wandb ì´ˆê¸°í™” ë¡œì§ (rank 0ë§Œ ì‹¤í–‰)
        local_rank = int(os.environ.get("LOCAL_RANK", -1))
        if self.config.get('use_wandb', False) and WANDB_AVAILABLE and local_rank <= 0:
            model_name = os.path.basename(self.config['model_name'])
            run_name = self.config.get('wandb_run_name') or f"{model_name}_{timestamp}"
            
            wandb.init(
                project=self.config.get('wandb_project', 'arms_qa'),
                name=run_name,
                config={
                    'model': self.config['model_name'],
                    'lora_r': self.config['lora_r'],
                    'batch_size': self.config['batch_size'],
                    'learning_rate': self.config['learning_rate'],
                    'max_length': self.config['max_length']
                },
                tags=['QA', 'PEFT', 'LoRA', 'Gemma3']
            )
            print(f"ğŸ“Š wandb ì´ˆê¸°í™” ì™„ë£Œ (Rank {local_rank}): {self.config.get('wandb_project')}/{run_name}")
        
        # ğŸ’¡ ì¤‘ìš”: DeepSpeed ZeRO-3ì—ì„œëŠ” TrainingArgumentsë¥¼ ë¨¼ì € ìƒì„±í•´ì•¼ í•¨
        training_args = self.setup_training_arguments(timestamp=timestamp)
        
        # ëª¨ë¸ ë¡œë“œ (TrainingArguments ìƒì„± í›„)
        model = self._load_model()
        
        # DataCollator ì„¤ì •
        collator = DataCollatorForCompletionOnly(
            tokenizer=self.tokenizer,
            max_length=self.config['max_length'],
        )
        
        # Callbacks ì„¤ì • - ì¢…í•© í‰ê°€ ë¹„í™œì„±í™” (eval_lossë§Œ ì‚¬ìš©)
        # Perplexity ë¡œê¹… ì½œë°±ì€ í•­ìƒ ì¶”ê°€
        callbacks = [PerplexityLoggingCallback()]
        metrics_callback = None
        
        # SFTTrainer ìƒì„± (peft_config ì œê±° - ì´ë¯¸ ëª¨ë¸ì— LoRA ì ìš©ë¨)
        trainer = SFTTrainer(
            model=model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            processing_class=self.tokenizer,
            formatting_func=self.formatting_func,
            data_collator=collator,
            callbacks=callbacks,
            # push_to_hub_token=None,  # TRL ì¼ë¶€ ë²„ì „ì—ì„œ ë¬´ì¡°ê±´ pop í•˜ë¯€ë¡œ ê¸°ë³¸ê°’ ì œê³µ
        )
        
        # í•™ìŠµ ì‹¤í–‰
        trainer.train()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥ ë° ê²°ê³¼ ë°˜í™˜ ë¡œì§ ìœ ì§€
        best_eval_loss = trainer.state.best_metric if hasattr(trainer.state, 'best_metric') and trainer.state.best_metric is not None else None
        model_folder_name = f"best_model_eval_loss_{best_eval_loss:.4f}" if best_eval_loss is not None else "best_model"
        
        final_output_dir = os.path.join(training_args.output_dir, model_folder_name)
        
        # DeepSpeed ZeRO-3: ëª¨ë“  rankì—ì„œ save_model í˜¸ì¶œ í•„ìš” (íŒŒë¼ë¯¸í„° ìˆ˜ì§‘)
        local_rank = int(os.environ.get("LOCAL_RANK", -1))
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘ (Rank {local_rank})...")
        trainer.save_model(final_output_dir)
        
        # tokenizer ì €ì¥ì€ rank 0ì—ì„œë§Œ
        if local_rank <= 0:
            self.tokenizer.save_pretrained(final_output_dir)
            print(f"âœ… í•™ìŠµ ì™„ë£Œ! Eval Loss ê¸°ì¤€ ëª¨ë¸: {final_output_dir}")
        else:
            print(f"âœ… Rank {local_rank}: ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        
        result = {
            'eval_loss_model': final_output_dir,
            'best_model': final_output_dir,
            'best_eval_loss': best_eval_loss,
            'trained_model': trainer.model,
            'tokenizer': self.tokenizer,
            'training_args': training_args
        }
        
        # wandb ì¢…ë£Œ (rank 0ë§Œ ì‹¤í–‰)
        local_rank = int(os.environ.get("LOCAL_RANK", -1))
        if self.config.get('use_wandb', False) and WANDB_AVAILABLE and local_rank <= 0:
            wandb.finish()
            print("ğŸ“Š wandb ë¡œê¹… ì™„ë£Œ")
            
        return result
    


def load_config(config_path: str = None) -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ (ë¡œì§ ìœ ì§€)"""
    # ... (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ...
    if config_path is None:
        config_path = "config/rag_peft_config.json"
    
    if not os.path.exists(config_path):
        print(f"âš ï¸ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return get_default_config()
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
        return config
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return get_default_config()


def get_default_config():
    """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
    return {
        # ëª¨ë¸ ì„¤ì •
        'model_name': '/home/rex/workspace/arms_qa/models/gemma-3-27b-it',
        
        # LoRA ì„¤ì •
        'lora_r': 16,
        'lora_alpha': 32,
        'lora_dropout': 0.05,
        'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
        
        # ë°ì´í„° ì„¤ì •
        'data_path': '/home/rex/workspace/arms_qa/data/train.jsonl',
        'eval_data_path': '/home/rex/workspace/arms_qa/data/test.jsonl',
        'max_length': 1024,  # QA íƒœìŠ¤í¬ëŠ” ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ í•„ìš”
        'max_samples': None,
        'validation_split': 0.1,
        
        # í•™ìŠµ ì„¤ì •
        'num_epochs': 3,
        'batch_size': 4,
        'gradient_accumulation_steps': 8,
        'learning_rate': 2e-4,
        'warmup_steps': 10,
        'logging_steps': 10,
        'save_steps': 100,
        'eval_steps': 100,
        
        # ì¶œë ¥ ì„¤ì •
        'output_dir': '/home/rex/workspace/arms_qa/output',
        'deepspeed': None,
        'local_rank': -1,
        'use_wandb': True,
        'wandb_project': 'arms_qa',
        'wandb_run_name': None,
    }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='QA PEFT í•™ìŠµ')
    parser.add_argument('--config', type=str, help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (JSON)')
    parser.add_argument('--model', type=str, help='ëª¨ë¸ ì´ë¦„')
    parser.add_argument('--data', type=str, help='ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--epochs', type=int, help='í•™ìŠµ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--eval_data', type=str, help='í‰ê°€ ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--batch_size', type=int, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--max_samples', type=int, help='ìµœëŒ€ ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--deepspeed', type=str, default=None, help='DeepSpeed config íŒŒì¼ ê²½ë¡œ (ë©€í‹° GPU í•™ìŠµ ì‹œ)')
    parser.add_argument('--local_rank', type=int, default=-1, help='DeepSpeed local rank (ìë™ ì„¤ì •ë¨)')
    
    args = parser.parse_args()
    
    config = load_config(args.config)
    
    if args.deepspeed:
        config['deepspeed'] = args.deepspeed
    if args.local_rank != -1:
        config['local_rank'] = args.local_rank
    
    # ëª…ë ¹í–‰ ì¸ìë¡œ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
    if args.model:
        config['model_name'] = args.model
    if args.data:
        config['data_path'] = args.data
    if args.eval_data:
        config['eval_data_path'] = args.eval_data
    if args.epochs:
        config['num_epochs'] = args.epochs
    if args.batch_size:
        config['batch_size'] = args.batch_size
    if args.max_samples:
        config['max_samples'] = args.max_samples
    
    print("ğŸš€ QA PEFT í•™ìŠµ ì‹œì‘")
    print("=" * 70)
    print(f"ëª¨ë¸: {config['model_name']}")
    print(f"ë°ì´í„°: {config['data_path']}")
    if config.get('eval_data_path'):
        print(f"í‰ê°€ ë°ì´í„°: {config['eval_data_path']}")
    print(f"ì—í¬í¬: {config['num_epochs']}")
    print(f"ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
    print(f"LoRA r: {config['lora_r']}")
    print(f"Max Length: {config['max_length']}")
    print("=" * 70)
    
    try:
        # ì „ì²´ í•™ìŠµ ì‹¤í–‰
        trainer = QAPEFTTrainer(config)
        trainer.setup_model_and_tokenizer()
        trainer.load_and_prepare_dataset()
        train_result = trainer.train()
        
        # ê²°ê³¼ ì²˜ë¦¬ ë¡œì§ ìœ ì§€
        if isinstance(train_result, dict):
            eval_loss_model = train_result.get('eval_loss_model')
            best_model = train_result.get('best_model')
            trained_model = train_result.get('trained_model')
            tokenizer = train_result.get('tokenizer')
            
            if not best_model:
                training_args = train_result.get('training_args')
                best_model = os.path.join(training_args.output_dir, "best_model") if training_args else os.path.join(config['output_dir'], "best_model")
            
            print(f"\nğŸ“Š í•™ìŠµ ê²°ê³¼:")
            if train_result.get('best_eval_loss'):
                print(f"  - Eval Loss: {train_result['best_eval_loss']:.4f} â†’ {eval_loss_model}")
            print(f"  - ëª¨ë¸ ì €ì¥: {best_model}")

            # í‰ê°€ ë©”ì„œë“œ ì œê±°ë¨ (ComprehensiveMetricsBestModelCallback ì œê±°ë¡œ ì¸í•´)
            eval_result = None
            
            result_file = best_model.replace('best_final', 'eval_result') + '.json'
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'config': {'epochs': config['num_epochs'], 'batch_size': config['batch_size'], 'lora_r': config['lora_r'], 'max_length': config['max_length']},
                    'eval_result': eval_result,
                    'train_result': {k:v for k,v in train_result.items() if k not in ['trained_model', 'tokenizer', 'training_args']} # ë¶ˆí•„ìš”í•œ ê°ì²´ ì œì™¸
                }, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“ í‰ê°€ ê²°ê³¼ ì €ì¥: {result_file}")
            
            # ì¶”ë¡  í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì œê±°ë¨ (ComprehensiveMetricsBestModelCallback ì œê±°ë¡œ ì¸í•´)
            print("âš ï¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ëŠ” ë³„ë„ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.")
        else:
            print("âš ï¸ í•™ìŠµ ê²°ê³¼ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
