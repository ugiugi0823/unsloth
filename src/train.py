#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""PEFT (LoRA) íŒŒì¸íŠœë‹ - SFTTrainer + DeepSpeed ZeRO + Unsloth"""

import os
import json
import torch
import argparse
import warnings
from datetime import datetime
from typing import Dict, List, Any

warnings.filterwarnings('ignore')

from unsloth import is_bfloat16_supported, FastLanguageModel
from unsloth.chat_templates import get_chat_template, train_on_responses_only
from datasets import Dataset
from trl import SFTTrainer, SFTConfig
from safetensors.torch import load_file, save_file
import wandb, weave
from unsloth import unsloth_train




def load_jsonl_or_json(path: str) -> List[Dict]:
    """JSONL ë˜ëŠ” JSON íŒŒì¼ ë¡œë“œ"""
    data = []
    if path.endswith('.jsonl'):
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
    else:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    return data


def normalize_example(item: Dict[str, Any]) -> Dict[str, Any] | None:
    """ë‹¤ì–‘í•œ ìŠ¤í‚¤ë§ˆë¥¼ {question, answer} í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    q, a = None, None
    
    # {"qas": [...]} í˜•ì‹
    if 'qas' in item and item['qas']:
        qa = item['qas'][0]
        q, a = qa.get('question'), qa.get('answer')
    
    # {"question": ..., "answer": ...} í˜•ì‹
    elif item.get('question') and item.get('answer'):
        q, a = item['question'], item['answer']
    
    # {"input": {"question": ...}, "output": {"answer": ...}} í˜•ì‹
    elif isinstance(item.get('input'), dict) and isinstance(item.get('output'), dict):
        q = item['input'].get('question')
        a = item['output'].get('answer')
    
    # ë¹ˆ ë¬¸ìì—´ ì²´í¬
    if q and a and str(q).strip() and str(a).strip():
        return {'question': str(q).strip(), 'answer': str(a).strip()}
    
    return None


def load_dataset_from_path(path: str, max_samples: int = None) -> Dataset:
    """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì •ê·œí™” (question, answer í•„ë“œ ìœ ì§€)"""
    data = load_jsonl_or_json(path)
    
    if max_samples and len(data) > max_samples:
        data = data[:max_samples]
    
    data = [normalize_example(ex) for ex in data]
    data = [ex for ex in data if ex is not None]
    
    return Dataset.from_list(data)


def create_formatting_func(tokenizer, max_seq_length=2048):
    """question, answer â†’ conversations â†’ text + input_ids ë³€í™˜"""
    def formatting_prompts_func(examples):
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_content = """ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ê·œì¹™:
1. ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ì—¬ ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ê¸°ìˆ ì ì¸ ìš©ì–´ê°€ ìˆë‹¤ë©´ ì •í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
3. ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."""
        
        # question, answerë¥¼ conversations í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        convos = []
        for question, answer in zip(examples.get('question', []), examples.get('answer', [])):
            # ë¹ˆ ê°’ ì²´í¬
            if not question or not answer or not str(question).strip() or not str(answer).strip():
                continue
            
            # Chat message êµ¬ì„± (system, user, assistant ìˆœì„œ)
            messages = [
                {"role": "system", "content": str(system_content)},
                {"role": "user", "content": str(question).strip()},
                {"role": "assistant", "content": str(answer).strip()}
            ]
            convos.append(messages)
        
        # conversationsë¥¼ textë¡œ ë³€í™˜ (removeprefixë¡œ bos í† í° ì œê±°)
        texts = []
        for convo in convos:
            text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)
            # bos í† í° ì œê±° (ìˆëŠ” ê²½ìš°)
            if text.startswith('<bos>'):
                text = text.removeprefix('<bos>')
            texts.append(text)
        
        return {"text": texts}
    return formatting_prompts_func


def load_config(config_path: str) -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    if not config_path or not os.path.exists(config_path):
        raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {config_path}")
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def main():
    parser = argparse.ArgumentParser(description='PEFT í•™ìŠµ')
    parser.add_argument('--config', type=str, required=True)
    parser.add_argument('--deepspeed', type=str)
    parser.add_argument('--local_rank', type=int, default=-1)
    args = parser.parse_args()
    
    config = load_config(args.config)
    if args.deepspeed:
        config['deepspeed'] = args.deepspeed
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    
    print("=" * 60)
    print(f"ğŸš€ PEFT í•™ìŠµ: {config['model_name']}")
    print(f"   ë°ì´í„°: {config['data_path']}")
    print(f"   ëª¨ë“œ: {'DeepSpeed' if config.get('deepspeed') else 'Default'}")
    print("=" * 60)
    
    # 1. ë°ì´í„°ì…‹ ë¡œë“œ
    train_dataset = load_dataset_from_path(config['data_path'], config.get('max_samples'))
    print(f"âœ… í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
    
    eval_dataset = None
    if config.get('eval_data_path') and os.path.exists(config['eval_data_path']):
        eval_dataset = load_dataset_from_path(config['eval_data_path'])
        print(f"âœ… í‰ê°€ ë°ì´í„°: {len(eval_dataset)}ê°œ")
    elif config.get('validation_split', 0) > 0:
        split = train_dataset.train_test_split(test_size=config['validation_split'], seed=42)
        train_dataset, eval_dataset = split['train'], split['test']
        print(f"âœ… í•™ìŠµ/ê²€ì¦ ë¶„í• : {len(train_dataset)}/{len(eval_dataset)}")
    
    # 2. ëª¨ë¸ ë¡œë”©
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {config['model_name']}")
    dtype = torch.bfloat16 if torch.cuda.is_available() and is_bfloat16_supported() else torch.float16
    max_seq_length = config.get('max_length', 2048)
    
    if config.get('full_finetuning', False):
        print("âœ… Full finetuning ëª¨ë“œ")
    else:
        print("âœ… PEFT ëª¨ë“œ")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=config['model_name'],
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=True,  # 4 bit quantization to reduce memory
        load_in_8bit=False, # [NEW!] A bit more accurate, uses 2x memory
        full_finetuning=config.get('full_finetuning', False), # [NEW!] We have full finetuning now!
        # attn_implementation="eager",
        attn_implementation="flash_attention_3",
        # device_map = "balanced"
    )
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    # 3. Chat template ì„¤ì •
    tokenizer = get_chat_template(tokenizer, chat_template="gemma-3")
    print(f"âœ… Tokenizer: eos={tokenizer.eos_token}, pad={tokenizer.pad_token}")
    
    # 4. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
    formatting_func = create_formatting_func(tokenizer, max_seq_length)
    train_dataset = train_dataset.map(formatting_func, batched=True)
    if eval_dataset:
        eval_dataset = eval_dataset.map(formatting_func, batched=True)
    print("âœ… ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì™„ë£Œ")
    
    # ë””ë²„ê·¸ ì¶œë ¥
    if local_rank <= 0 and len(train_dataset) > 0:
        print(f"\nğŸ” ì²« ë²ˆì§¸ ìƒ˜í”Œ:\n{train_dataset[0]['text'][:]}\n")
    
    # 5. LoRA ì„¤ì •
    model = FastLanguageModel.get_peft_model(
        model,
        r=config['lora_r'],
        lora_alpha=config['lora_alpha'],
        lora_dropout=config.get('lora_dropout', 0.05),
        target_modules=config['target_modules'],
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
    )
    if hasattr(model, 'print_trainable_parameters'):
        model.print_trainable_parameters()
    
    # 6. wandb
    if config.get('use_wandb') and local_rank <= 0:
        weave.init(config.get('wandb_project', 'full_finetuning'))
        wandb.init(project=config.get('wandb_project', 'full_finetuning'), name=f"{os.path.basename(config['model_name'])}_{timestamp}", config=config)
    
    # 7. Warmup ê³„ì‚°
    num_gpus = int(os.environ.get("WORLD_SIZE", 1))
    global_batch_size = config['batch_size'] * config.get('gradient_accumulation_steps', 1) * num_gpus
    
    # Total_Steps = (N Ã— Epochs) / (Batch_Size Ã— Grad_Accumulation)
    N = len(train_dataset)
    Epochs = config['num_epochs']
    Batch_Size = config['batch_size']
    Grad_Accumulation = config.get('gradient_accumulation_steps', 1)
    
    total_steps = int((N * Epochs) / (Batch_Size * Grad_Accumulation))
    save_steps = int(total_steps * 0.1)
    eval_steps = int(total_steps * 0.1) if eval_dataset else None
    logging_steps = int(total_steps * 0.01)
    warmup_steps = int(total_steps * config.get('warmup_ratio', 0.05))
    
    if local_rank <= 0:
        print(f"ğŸ“Š GPU: {num_gpus}, ë°°ì¹˜: {global_batch_size}, ìŠ¤í…: {total_steps}, Warmup: {warmup_steps}, Logging: {logging_steps}, Save/Eval: {save_steps}")
    
    # 8. SFTConfig
    # ëª¨ë¸ ì´ë¦„ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ì‚¬ìš© (ì˜ˆ: "Qwen/Qwen3-Coder-30B-A3B-Instruct" -> "Qwen3-Coder-30B-A3B-Instruct")
    model_name_safe = config['model_name'].split('/')[-1]
    
    if config.get('full_finetuning', False):
        output_dir = os.path.join(config['output_dir'], model_name_safe, f"full_{timestamp}")
    else:
        output_dir = os.path.join(config['output_dir'], model_name_safe, f"lora_{timestamp}")
    
    training_args = SFTConfig(
        output_dir=output_dir,
        dataset_text_field="text",
        num_train_epochs=config['num_epochs'],
        per_device_train_batch_size=config['batch_size'],
        per_device_eval_batch_size=config['batch_size'],
        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 1),
        learning_rate=config['learning_rate'],
        warmup_steps=warmup_steps,
        logging_steps=logging_steps,
        save_steps=save_steps,
        eval_steps=eval_steps,
        eval_strategy="steps" if eval_dataset else "no",
        save_strategy="steps",
        bf16=True,
        optim=config.get('optim', 'adamw_torch'),
        lr_scheduler_type=config.get('lr_scheduler_type', 'cosine'),
        weight_decay=config.get('weight_decay', 0.01),
        max_grad_norm=config.get('max_grad_norm', 1.0),
        save_total_limit=3,
        dataloader_drop_last=True,
        packing=True,
        gradient_checkpointing=False,
        deepspeed=config.get('deepspeed'),
        local_rank=args.local_rank,
        report_to="wandb" if config.get('use_wandb') else None,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        load_best_model_at_end=True,
    )
    
    # 9. Trainer
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
    )
    
    # 10. Response-only í•™ìŠµ
    trainer = train_on_responses_only(trainer, instruction_part = "<start_of_turn>user\n",response_part = "<start_of_turn>model\n")
    # trainer = train_on_responses_only(trainer, instruction_part="<|im_start|>user\n", response_part="<|im_start|>assistant\n")
    print("âœ… Response-only í•™ìŠµ ëª¨ë“œ")
    
    # 11. í•™ìŠµ
    # trainer.train()
    unsloth_train(trainer)
    
    # 12. ì €ì¥
    if config.get('merge_weights', False):
        final_dir = os.path.join(output_dir, "final_model")
        if local_rank <= 0:
            model.save_pretrained_merged(final_dir, tokenizer, save_method = "merged_16bit")
            print(f"\nâœ… ì™„ë£Œ! ëª¨ë¸: full_finetuning")
            print(f"\nâœ… ê²½ë¡œ: {final_dir}")
    else:
        final_dir = os.path.join(output_dir, "final_model_peft")
        if local_rank <= 0:
            model.save_pretrained(final_dir)
            # ì €ì¥ í›„ bfloat16 ë³€í™˜
            # adapter_path = os.path.join(final_dir, "adapter_model.safetensors")
            # if os.path.exists(adapter_path):
            #     tensors = load_file(adapter_path)
            #     tensors = {k: v.to(torch.bfloat16) for k, v in tensors.items()}
            #     save_file(tensors, adapter_path)
            tokenizer.save_pretrained(final_dir)
            print(f"\nâœ… ì™„ë£Œ! ëª¨ë¸: final_model_peft")
            print(f"\nâœ… ê²½ë¡œ: {final_dir}")
        
        
    
    
    if config.get('use_wandb') and local_rank <= 0:
        wandb.finish()


if __name__ == "__main__":
    main()
