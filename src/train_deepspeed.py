#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PEFT (LoRA) íŒŒì¸íŠœë‹ - SFTTrainer + DeepSpeed ZeRO

deepspeed --num_gpus=4 train_deepspeed.py --config config.json
"""

import os
import json
import torch
import argparse
import warnings
from datetime import datetime
from typing import Dict, List, Any

warnings.filterwarnings('ignore')

from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig, TaskType, get_peft_model

import wandb


# ============================================================================
# Data Utils
# ============================================================================

def load_jsonl_or_json(path: str) -> List[Dict]:
    """JSONL ë˜ëŠ” JSON íŒŒì¼ ë¡œë“œ"""
    data = []
    if path.endswith('.jsonl'):
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
    else:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    return data


def normalize_example(item: Dict[str, Any]) -> Dict[str, Any] | None:
    """ë‹¤ì–‘í•œ ìŠ¤í‚¤ë§ˆë¥¼ {question, answer} í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    q, a = None, None
    
    # {"qas": [...]} í˜•ì‹
    if 'qas' in item and item['qas']:
        qa = item['qas'][0]
        q, a = qa.get('question'), qa.get('answer')
    
    # {"question": ..., "answer": ...} í˜•ì‹
    elif item.get('question') and item.get('answer'):
        q, a = item['question'], item['answer']
    
    # {"input": {"question": ...}, "output": {"answer": ...}} í˜•ì‹
    elif isinstance(item.get('input'), dict) and isinstance(item.get('output'), dict):
        q = item['input'].get('question')
        a = item['output'].get('answer')
    
    # ë¹ˆ ë¬¸ìì—´ ì²´í¬
    if q and a and str(q).strip() and str(a).strip():
        return {'question': str(q).strip(), 'answer': str(a).strip()}
    
    return None


def load_dataset_from_path(path: str, max_samples: int = None) -> Dataset:
    """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì •ê·œí™” (question, answer í•„ë“œ ìœ ì§€)"""
    data = load_jsonl_or_json(path)
    
    if max_samples and len(data) > max_samples:
        data = data[:max_samples]
    
    data = [normalize_example(ex) for ex in data]
    data = [ex for ex in data if ex is not None]
    
    return Dataset.from_list(data)


def create_formatting_func(tokenizer):
    """Gemma3 Chat í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” formatting_func ìƒì„± (ë‹¨ì¼ ë¬¸ìì—´ ë°˜í™˜)"""
    def formatting_func(example):
        question = example.get('question', '')
        answer = example.get('answer', '')
        
        # ë¹ˆ ê°’ ì²´í¬
        if not question or not answer or not question.strip() or not answer.strip():
            return ""
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬
        system_content = """ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ê·œì¹™:
1. ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ì—¬ ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ê¸°ìˆ ì ì¸ ìš©ì–´ê°€ ìˆë‹¤ë©´ ì •í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
3. ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."""
        
        # ìœ ì € ì»¨í…ì¸ ëŠ” ì§ˆë¬¸ë§Œ í¬í•¨ (í…ŒìŠ¤íŠ¸ ì‹œì™€ ë™ì¼í•˜ê²Œ)
        user_content = question
        
        # Chat message êµ¬ì„± (system, user, assistant ìˆœì„œ)
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": answer}
        ]
        
        # tokenizer.apply_chat_templateìœ¼ë¡œ ë‹¨ì¼ ë¬¸ìì—´ ë°˜í™˜
        formatted_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        
        return formatted_text
    
    return formatting_func


# ============================================================================
# Config & Main
# ============================================================================

def load_config(config_path: str) -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    if not config_path or not os.path.exists(config_path):
        raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {config_path}")
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def main():
    parser = argparse.ArgumentParser(description='PEFT í•™ìŠµ')
    parser.add_argument('--config', type=str, required=True, help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--deepspeed', type=str, help='DeepSpeed config ê²½ë¡œ')
    parser.add_argument('--local_rank', type=int, default=-1)
    args = parser.parse_args()
    
    config = load_config(args.config)
    if args.deepspeed:
        config['deepspeed'] = args.deepspeed
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    
    print("=" * 60)
    print(f"ğŸš€ PEFT í•™ìŠµ: {config['model_name']}")
    print(f"   ë°ì´í„°: {config['data_path']}")
    print(f"   ëª¨ë“œ: {'DeepSpeed' if config.get('deepspeed') else 'Default'}")
    print("=" * 60)
    
    # 1. í† í¬ë‚˜ì´ì €
    tokenizer = AutoTokenizer.from_pretrained(
        config['model_name'],
        trust_remote_code=True,
        padding_side="right",
        use_fast=False
    )
    
    # Gemma3Processorì¸ ê²½ìš° ë‚´ë¶€ í† í¬ë‚˜ì´ì €ì—ì„œ eos_token ê°€ì ¸ì˜¤ê¸°
    inner_tokenizer = getattr(tokenizer, 'tokenizer', tokenizer)
    
    # eos_token ì„¤ì • (setattrë¡œ ê°•ì œ ì„¤ì •)
    if not hasattr(tokenizer, 'eos_token') or tokenizer.eos_token is None:
        if hasattr(inner_tokenizer, 'eos_token') and inner_tokenizer.eos_token is not None:
            setattr(tokenizer, 'eos_token', inner_tokenizer.eos_token)
        else:
            setattr(tokenizer, 'eos_token', "<eos>")
    
    # pad_token ì„¤ì • (setattrë¡œ ê°•ì œ ì„¤ì •)
    if not hasattr(tokenizer, 'pad_token') or tokenizer.pad_token is None:
        if hasattr(inner_tokenizer, 'pad_token') and inner_tokenizer.pad_token is not None:
            setattr(tokenizer, 'pad_token', inner_tokenizer.pad_token)
        else:
            setattr(tokenizer, 'pad_token', tokenizer.eos_token)
    
    print(f"âœ… Tokenizer type: {type(tokenizer).__name__}")
    print(f"   eos_token: {tokenizer.eos_token}, pad_token: {tokenizer.pad_token}")
    
    # 2. ë°ì´í„°ì…‹ (question, answer í•„ë“œ ìœ ì§€)
    train_dataset = load_dataset_from_path(
        config['data_path'],
        config.get('max_samples')
    )
    print(f"âœ… í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
    
    eval_dataset = None
    if config.get('eval_data_path') and os.path.exists(config['eval_data_path']):
        eval_dataset = load_dataset_from_path(config['eval_data_path'])
        print(f"âœ… í‰ê°€ ë°ì´í„°: {len(eval_dataset)}ê°œ")
    elif config.get('validation_split', 0) > 0:
        split = train_dataset.train_test_split(test_size=config['validation_split'], seed=42)
        train_dataset, eval_dataset = split['train'], split['test']
        print(f"âœ… í•™ìŠµ/ê²€ì¦ ë¶„í• : {len(train_dataset)}/{len(eval_dataset)}")
    
    # 3. ëª¨ë¸ + LoRA
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {config['model_name']}")
    model = AutoModelForCausalLM.from_pretrained(
        config['model_name'],
        attn_implementation="eager",
        # attn_implementation="flash_attention_2",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map=None,
        low_cpu_mem_usage=True,
        # is_training=True, 
    )
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    model.config.use_cache = False
    
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=config['lora_r'],
        lora_alpha=config['lora_alpha'],
        lora_dropout=config.get('lora_dropout', 0.05),
        target_modules=config['target_modules'],
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    model.enable_input_require_grads()
    
    # 4. wandb
    if config.get('use_wandb') and local_rank <= 0:
        wandb.init(
            project=config.get('wandb_project', 'peft'),
            name=f"{os.path.basename(config['model_name'])}_{timestamp}",
            config=config
        )
    
    # ============================================================================
    # 5. Warmup Steps ìë™ ê³„ì‚°
    # ============================================================================
    WARMUP_RATIO = config.get('warmup_ratio', 0.05)  # configì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ 5%
    
    # GPU ê°œìˆ˜ (DeepSpeed í™˜ê²½ì—ì„œëŠ” WORLD_SIZE ì‚¬ìš©)
    num_gpus = int(os.environ.get("WORLD_SIZE", 1))
    micro_batch_size = config['batch_size']
    grad_accum_steps = config.get('gradient_accumulation_steps', 1)
    num_epochs = config['num_epochs']
    total_samples = len(train_dataset)
    
    # ê¸€ë¡œë²Œ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
    global_batch_size = micro_batch_size * grad_accum_steps * num_gpus
    
    # ì´ í•™ìŠµ ìŠ¤í… ìˆ˜ ê³„ì‚° (ì˜¬ë¦¼ ì²˜ë¦¬)
    if global_batch_size == 0:
        total_steps = 0
    else:
        steps_per_epoch = (total_samples + global_batch_size - 1) // global_batch_size
        total_steps = steps_per_epoch * num_epochs
    
    # Warmup Steps ê³„ì‚°
    calculated_warmup_steps = int(total_steps * WARMUP_RATIO)
    
    if local_rank <= 0:
        print(f"--- Warmup Steps ìë™ ê³„ì‚° ê²°ê³¼ ---")
        print(f"  GPU ê°œìˆ˜: {num_gpus}")
        print(f"  ê¸€ë¡œë²Œ ë°°ì¹˜ í¬ê¸°: {global_batch_size}")
        print(f"  ì´ í•™ìŠµ ìŠ¤í… ìˆ˜: {total_steps} (Epoch: {num_epochs})")
        print(f"  Warmup Steps: {calculated_warmup_steps} ({WARMUP_RATIO*100}%)")
        print(f"------------------------------------")
    
    # 6. SFTConfig
    output_dir = os.path.join(config['output_dir'], f"peft_{timestamp}")
    
    training_args = SFTConfig(
        output_dir=output_dir,
        num_train_epochs=config['num_epochs'],
        per_device_train_batch_size=config['batch_size'],
        per_device_eval_batch_size=config['batch_size'],
        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 1),
        learning_rate=config['learning_rate'],
        warmup_steps=calculated_warmup_steps,
        logging_steps=config.get('logging_steps', 10),
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        load_best_model_at_end=True,
        save_steps=config.get('save_steps', 100),
        eval_steps=config.get('eval_steps', 100) if eval_dataset else None,
        eval_strategy="steps" if eval_dataset else "no",
        save_strategy="steps",
        bf16=True,
        optim=config.get('optim', 'adamw_torch'),
        lr_scheduler_type=config.get('lr_scheduler_type', 'cosine'),
        loss_type=config.get('loss_type', 'dft'),
        weight_decay=0.01,
        max_grad_norm=config.get('max_grad_norm', 1.0),
        save_total_limit=3,
        dataloader_drop_last=True,
        group_by_length=False,
        packing=True,
        remove_unused_columns=True,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False}, 
        deepspeed=config.get('deepspeed'),
        local_rank=args.local_rank,
        report_to="wandb" if config.get('use_wandb') else None,
        max_length=config.get('max_length', 2048),
        # use_liger_kernel=True
    )
    
    # ê³ ì • ê¸¸ì´ íŒ¨ë”© DataCollator (DeepSpeed ZeRO-3 í˜¸í™˜ì„±)
    # max_seq_len = config.get('max_length', 2048)
    # data_collator = DataCollatorForSeq2Seq(
    #     tokenizer,
    #     padding="max_length",      # í•µì‹¬: ë¬´ì¡°ê±´ max_lengthë¡œ íŒ¨ë”©
    #     max_length=max_seq_len,    # ê³ ì • ê¸¸ì´ ì§€ì •
    #     pad_to_multiple_of=8,      # ì„±ëŠ¥ ìµœì í™”
    #     return_tensors="pt"
    # )
    
    # 7. í•™ìŠµ (formatting_func ì‚¬ìš© - ë‹¨ì¼ ë¬¸ìì—´ ë°˜í™˜)
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        # tokenizer=tokenizer,
        processing_class=tokenizer,
        # data_collator=data_collator,  # ê³ ì • ê¸¸ì´ collator ì „ë‹¬
        formatting_func=create_formatting_func(tokenizer),
        # model_init_kwargs={"_compute_loss": True},
    )
    
    trainer.train()
    
    # 8. ì €ì¥
    final_dir = os.path.join(output_dir, "final_model")
    trainer.save_model(final_dir)
    if local_rank <= 0:
        tokenizer.save_pretrained(final_dir)
        print(f"\nâœ… ì™„ë£Œ! ëª¨ë¸: {final_dir}")
    
    if config.get('use_wandb') and local_rank <= 0:
        wandb.finish()


if __name__ == "__main__":
    main()
